{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import ngrams\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you have to get a sentiment score from a movie review.\n",
    "\n",
    "Simple way: two lists of words, one positive, one negative. Don't even need a model, if the number of positive words is higher, it is a positive sentiment for the movie review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentiment(sentence, pos, neg):\n",
    "    sentence = sentence.split()\n",
    "    \n",
    "    # make a set of the words in the sentence\n",
    "    sentence = set(sentence)\n",
    "    \n",
    "    num_common_pos = len(sentence.intersection(pos))\n",
    "    \n",
    "    num_common_neg = len(sentence.intersection(neg))\n",
    "    \n",
    "    if num_common_pos > num_common_neg:\n",
    "        return \"positive\"\n",
    "    if num_common_pos < num_common_neg:\n",
    "        return \"negative\"\n",
    "    return \"neutral\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"hi, how are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi,', 'how', 'are', 'you?']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of words\n",
    "We create a huge sparse matrix that stores counts of all the words in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 25 stored elements and shape (5, 23)>\n",
      "  Coords\tValues\n",
      "  (0, 2)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 22)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 22)\t2\n",
      "  (2, 0)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 22)\t1\n",
      "  (3, 12)\t1\n",
      "  (3, 15)\t1\n",
      "  (3, 16)\t1\n",
      "  (3, 18)\t1\n",
      "  (3, 20)\t1\n",
      "  (4, 21)\t1\n"
     ]
    }
   ],
   "source": [
    "# create a corpus of sentences\n",
    "corpus = [\n",
    "\"hello, how are you?\",\n",
    "\"im getting bored at home. And you? What do you think?\",\n",
    "\"did you know about counts\",\n",
    "\"let's see if this works!\",\n",
    "\"YES!!!!\"\n",
    "]\n",
    "\n",
    "# initialize CountVectorizer\n",
    "ctv = CountVectorizer()\n",
    "\n",
    "# fit the vectorizer on corpus\n",
    "ctv.fit(corpus)\n",
    "corpus_transformed = ctv.transform(corpus)\n",
    "\n",
    "print(corpus_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 14, ',': 2, 'how': 16, 'are': 7, 'you': 27, '?': 4, 'im': 18, 'getting': 13, 'bored': 9, 'at': 8, 'home': 15, '.': 3, 'and': 6, 'what': 24, 'do': 12, 'think': 22, 'did': 11, 'know': 19, 'about': 5, 'counts': 10, 'let': 20, \"'s\": 1, 'see': 21, 'if': 17, 'this': 23, 'works': 25, '!': 0, 'yes': 26}\n"
     ]
    }
   ],
   "source": [
    "# create a corpus of sentences\n",
    "corpus = [\n",
    "\"hello, how are you?\",\n",
    "\"im getting bored at home. And you? What do you think?\",\n",
    "\"did you know about counts\",\n",
    "\"let's see if this works!\",\n",
    "\"YES!!!!\"\n",
    "]\n",
    "# initialize CountVectorizer with word_tokenize from nltk\n",
    "# as the tokenizer\n",
    "ctv = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "# fit the vectorizer on corpus\n",
    "ctv.fit(corpus)\n",
    "corpus_transformed = ctv.transform(corpus)\n",
    "print(ctv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a sparse matrix by using all the sentences in IMDB sataset and build a model. Use accuracy as the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Accuracy = 0.8943\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Accuracy = 0.8843\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 2\n",
      "Accuracy = 0.8895\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 3\n",
      "Accuracy = 0.8875\n",
      "\n",
      "Fold: 4\n",
      "Accuracy = 0.9005\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # read the training data\n",
    "    df = pd.read_csv(r\"C:\\Users\\Chand\\OneDrive\\桌面\\ML Project\\NLP\\imdb.csv\")\n",
    "\n",
    "    # map positive to 1 and negative to 0\n",
    "    df.sentiment = df.sentiment.apply(\n",
    "    lambda x: 1 if x == \"positive\" else 0)\n",
    "\n",
    "    df[\"kfold\"] = -1\n",
    "\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # fetch labels\n",
    "    y = df.sentiment.values\n",
    "\n",
    "    # initiate the kfold class from model_selection module\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # fill the new kfold column\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "        df.loc[v_, 'kfold'] = f\n",
    "\n",
    "    for fold_ in range(5):\n",
    "        train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
    "        test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
    "\n",
    "        # initialize CountVectorizer with NLTK's word_tokenize\n",
    "        # function as the tokenizer\n",
    "        ctv = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "\n",
    "        # fit count vectorizer on training data reviews\n",
    "        ctv.fit(train_df.review)\n",
    "\n",
    "        # transform training and validation data reviews\n",
    "        xtrain = ctv.transform(train_df.review)\n",
    "        xtest = ctv.transform(test_df.review)\n",
    "\n",
    "        # initialize logistic regression model\n",
    "        model = linear_model.LogisticRegression()\n",
    "\n",
    "        # fit the model on training data reviews and sentiment\n",
    "        model.fit(xtrain, train_df.sentiment)\n",
    "\n",
    "        # make predictions on test data\n",
    "        # threshold for predictions is 0.5\n",
    "        preds = model.predict(xtest)\n",
    "\n",
    "        # calculate accuracy\n",
    "        accuracy = metrics.accuracy_score(test_df.sentiment, preds)\n",
    "\n",
    "        print(f\"Fold: {fold_}\")\n",
    "        print(f\"Accuracy = {accuracy}\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are at the 89 percent accuracy but the model took a long time to train. Can improve time by using naive bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Accuracy = 0.8494\n",
      "\n",
      "Fold: 1\n",
      "Accuracy = 0.8412\n",
      "\n",
      "Fold: 2\n",
      "Accuracy = 0.843\n",
      "\n",
      "Fold: 3\n",
      "Accuracy = 0.8442\n",
      "\n",
      "Fold: 4\n",
      "Accuracy = 0.8423\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Read the training data\n",
    "    df = pd.read_csv(r\"C:\\Users\\Chand\\OneDrive\\桌面\\ML Project\\NLP\\imdb.csv\")\n",
    "\n",
    "    # Map positive to 1 and negative to 0\n",
    "    df.sentiment = df.sentiment.apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "\n",
    "    # Add and shuffle kfold column\n",
    "    df[\"kfold\"] = -1\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Fetch labels\n",
    "    y = df.sentiment.values\n",
    "\n",
    "    # Initiate StratifiedKFold\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Assign folds\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "        df.loc[v_, 'kfold'] = f\n",
    "\n",
    "    # 5-fold cross-validation\n",
    "    for fold_ in range(5):\n",
    "        train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
    "        test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
    "\n",
    "        # Initialize CountVectorizer with NLTK's word_tokenize function\n",
    "        ctv = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "\n",
    "        # Fit CountVectorizer on training data reviews\n",
    "        ctv.fit(train_df.review)\n",
    "\n",
    "        # Transform training and validation data reviews\n",
    "        xtrain = ctv.transform(train_df.review)\n",
    "        xtest = ctv.transform(test_df.review)\n",
    "\n",
    "        # Initialize Multinomial Naive Bayes model\n",
    "        model = naive_bayes.MultinomialNB()\n",
    "\n",
    "        # Fit the model on training data reviews and sentiment\n",
    "        model.fit(xtrain, train_df.sentiment)\n",
    "\n",
    "        # Make predictions on test data\n",
    "        preds = model.predict(xtest)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = metrics.accuracy_score(test_df.sentiment, preds)\n",
    "\n",
    "        print(f\"Fold: {fold_}\")\n",
    "        print(f\"Accuracy = {accuracy}\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams: combinations of words in order. And may increase model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hi', ',', 'how'), (',', 'how', 'are'), ('how', 'are', 'you'), ('are', 'you', '?')]\n"
     ]
    }
   ],
   "source": [
    "# let's see 3 grams\n",
    "N = 3\n",
    "# input sentence\n",
    "sentence = \"hi, how are you?\"\n",
    "# tokenized sentence\n",
    "tokenized_sentence = word_tokenize(sentence)\n",
    "# generate n_grams\n",
    "n_grams = list(ngrams(tokenized_sentence, N))\n",
    "print(n_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(\n",
    "tokenizer=word_tokenize,\n",
    "token_pattern=None,\n",
    "ngram_range=(1, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Accuracy = 0.8878\n",
      "\n",
      "Fold: 1\n",
      "Accuracy = 0.8904\n",
      "\n",
      "Fold: 2\n",
      "Accuracy = 0.8895\n",
      "\n",
      "Fold: 3\n",
      "Accuracy = 0.895\n",
      "\n",
      "Fold: 4\n",
      "Accuracy = 0.8864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Read the training data\n",
    "    df = pd.read_csv(r\"C:\\Users\\Chand\\OneDrive\\桌面\\ML Project\\NLP\\imdb.csv\")\n",
    "\n",
    "    # Map positive to 1 and negative to 0\n",
    "    df.sentiment = df.sentiment.apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "\n",
    "    # Add and shuffle kfold column\n",
    "    df[\"kfold\"] = -1\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Fetch labels\n",
    "    y = df.sentiment.values\n",
    "\n",
    "    # Initiate StratifiedKFold\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Assign folds\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "        df.loc[v_, 'kfold'] = f\n",
    "\n",
    "    # 5-fold cross-validation\n",
    "    for fold_ in range(5):\n",
    "        train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
    "        test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
    "\n",
    "        # Initialize TfidfVectorizer with NLTK's word_tokenize function\n",
    "        tfidf_vec = TfidfVectorizer(\n",
    "            tokenizer=word_tokenize,\n",
    "            token_pattern=None,\n",
    "            ngram_range=(1, 3)  # Includes unigrams, bigrams, and trigrams\n",
    "        )\n",
    "\n",
    "        # Fit TfidfVectorizer on training data reviews\n",
    "        tfidf_vec.fit(train_df.review)\n",
    "\n",
    "        # Transform training and validation data reviews\n",
    "        xtrain = tfidf_vec.transform(train_df.review)\n",
    "        xtest = tfidf_vec.transform(test_df.review)\n",
    "\n",
    "        # Initialize Multinomial Naive Bayes model\n",
    "        model = naive_bayes.MultinomialNB()\n",
    "\n",
    "        # Fit the model on training data reviews and sentiment\n",
    "        model.fit(xtrain, train_df.sentiment)\n",
    "\n",
    "        # Make predictions on test data\n",
    "        preds = model.predict(xtest)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = metrics.accuracy_score(test_df.sentiment, preds)\n",
    "\n",
    "        print(f\"Fold: {fold_}\")\n",
    "        print(f\"Accuracy = {accuracy}\")\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming and lemmatization: reduce a word to its smallest form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word=worders\n",
      "stemmed_word=worder\n",
      "lemma=worders\n",
      "\n",
      "word=drove\n",
      "stemmed_word=drove\n",
      "lemma=drove\n",
      "\n",
      "word=cats\n",
      "stemmed_word=cat\n",
      "lemma=cat\n",
      "\n",
      "word=running\n",
      "stemmed_word=run\n",
      "lemma=running\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# initialize Snowball Stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "words = [\"worders\", \"drove\", \"cats\", \"running\"]\n",
    "\n",
    "for word in words:\n",
    "    print(f\"word={word}\")\n",
    "    print(f\"stemmed_word={stemmer.stem(word)}\")\n",
    "    print(f\"lemma={lemmatizer.lemmatize(word)}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if adding stemming can increase accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Accuracy = 0.8922\n",
      "\n",
      "Fold: 1\n",
      "Accuracy = 0.888\n",
      "\n",
      "Fold: 2\n",
      "Accuracy = 0.8964\n",
      "\n",
      "Fold: 3\n",
      "Accuracy = 0.8886\n",
      "\n",
      "Fold: 4\n",
      "Accuracy = 0.8907\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize lemmatizer and stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Function to preprocess text using lemmatization and stemming\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Lemmatize and stem each token\n",
    "    processed_tokens = [stemmer.stem(lemmatizer.lemmatize(token)) for token in tokens]\n",
    "    # Join the processed tokens back into a single string\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the training data\n",
    "    df = pd.read_csv(r\"C:\\Users\\Chand\\OneDrive\\桌面\\ML Project\\NLP\\imdb.csv\")\n",
    "\n",
    "    # Map positive to 1 and negative to 0\n",
    "    df.sentiment = df.sentiment.apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "\n",
    "    # Add and shuffle kfold column\n",
    "    df[\"kfold\"] = -1\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Fetch labels\n",
    "    y = df.sentiment.values\n",
    "\n",
    "    # Initiate StratifiedKFold\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Assign folds\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "        df.loc[v_, 'kfold'] = f\n",
    "\n",
    "    # 5-fold cross-validation\n",
    "    for fold_ in range(5):\n",
    "        train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
    "        test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
    "\n",
    "        # Preprocess the review text using lemmatization and stemming\n",
    "        train_df[\"review\"] = train_df[\"review\"].apply(preprocess_text)\n",
    "        test_df[\"review\"] = test_df[\"review\"].apply(preprocess_text)\n",
    "\n",
    "        # Initialize TfidfVectorizer with NLTK's word_tokenize function\n",
    "        tfidf_vec = TfidfVectorizer(\n",
    "            tokenizer=word_tokenize,\n",
    "            token_pattern=None,\n",
    "            ngram_range=(1, 3)  # Includes unigrams, bigrams, and trigrams\n",
    "        )\n",
    "\n",
    "        # Fit TfidfVectorizer on training data reviews\n",
    "        tfidf_vec.fit(train_df.review)\n",
    "\n",
    "        # Transform training and validation data reviews\n",
    "        xtrain = tfidf_vec.transform(train_df.review)\n",
    "        xtest = tfidf_vec.transform(test_df.review)\n",
    "\n",
    "        # Initialize Multinomial Naive Bayes model\n",
    "        model = naive_bayes.MultinomialNB()\n",
    "\n",
    "        # Fit the model on training data reviews and sentiment\n",
    "        model.fit(xtrain, train_df.sentiment)\n",
    "\n",
    "        # Make predictions on test data\n",
    "        preds = model.predict(xtest)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = metrics.accuracy_score(test_df.sentiment, preds)\n",
    "\n",
    "        print(f\"Fold: {fold_}\")\n",
    "        print(f\"Accuracy = {accuracy}\")\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic extraction\n",
    "- Can be done using non-negaitve matric factorization (NMF) or latent semantic analysis(LSA).\n",
    "This can reduce the data to a given number of components.\n",
    "\n",
    "- TF-IDF (Term Frequency-Inverse Document Frequency) features are numerical representations of words or phrases in a text corpus that reflect their importance in a document relative to the entire corpus. These features are commonly used in natural language processing (NLP) and machine learning tasks involving text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', ',', '.', 'a', 'and']\n"
     ]
    }
   ],
   "source": [
    "# let's read only 10k samples from the data.\n",
    "corpus = pd.read_csv(r\"C:\\Users\\Chand\\OneDrive\\桌面\\ML Project\\NLP\\imdb.csv\", nrows=10000)\n",
    "corpus = corpus.review.values\n",
    "\n",
    "# initialize TfidfVectorizer\n",
    "tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "\n",
    "# fit the vectorizer on corpus\n",
    "tfv.fit(corpus)\n",
    "\n",
    "# transform the corpus\n",
    "corpus_transformed = tfv.transform(corpus)\n",
    "\n",
    "# initialize SVD with 10 components\n",
    "svd = decomposition.TruncatedSVD(n_components=10)\n",
    "\n",
    "# fit SVD\n",
    "corpus_svd = svd.fit(corpus_transformed)\n",
    "\n",
    "sample_index = 0\n",
    "feature_scores = dict(\n",
    "    zip(\n",
    "        tfv.get_feature_names_out(),\n",
    "        corpus_svd.components_[sample_index]\n",
    "    )\n",
    ")\n",
    "\n",
    "# once we have the feature scores, we can sort them in descending order\n",
    "N = 5\n",
    "print(sorted(feature_scores, key=feature_scores.get, reverse=True)[:N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clean text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    # remove punctuation\n",
    "    s = s.split()\n",
    "    \n",
    "    # join tokens by single space. this will remove all kinds of weird space characters\n",
    "    s = \" \".join(s)\n",
    "    \n",
    "    # remove all punctuation\n",
    "    s = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", s)\n",
    "    \n",
    "    return s\n",
    "\n",
    "# This function will convert a string like “hi, how are you????” to “hi how are you”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'a', 'and', 'of', 'to']\n"
     ]
    }
   ],
   "source": [
    "corpus = pd.read_csv(r\"C:\\Users\\Chand\\OneDrive\\桌面\\ML Project\\NLP\\imdb.csv\", nrows=10000)\n",
    "corpus.loc[:, \"review\"] = corpus.review.apply(clean_text)\n",
    "corpus = corpus.review.values\n",
    "\n",
    "\n",
    "# then rerun the code for training the model\n",
    "# initialize TfidfVectorizer\n",
    "tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "\n",
    "# fit the vectorizer on corpus\n",
    "tfv.fit(corpus)\n",
    "\n",
    "# transform the corpus\n",
    "corpus_transformed = tfv.transform(corpus)\n",
    "\n",
    "# initialize SVD with 10 components\n",
    "svd = decomposition.TruncatedSVD(n_components=10)\n",
    "\n",
    "# fit SVD\n",
    "corpus_svd = svd.fit(corpus_transformed)\n",
    "\n",
    "sample_index = 0\n",
    "feature_scores = dict(\n",
    "    zip(\n",
    "        tfv.get_feature_names_out(),\n",
    "        corpus_svd.components_[sample_index]\n",
    "    )\n",
    ")\n",
    "\n",
    "# once we have the feature scores, we can sort them in descending order\n",
    "N = 5\n",
    "print(sorted(feature_scores, key=feature_scores.get, reverse=True)[:N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings\n",
    "- Meaning: representation of words into vectors.\n",
    "- Word2Vec is the oldest approach to convert words into vectors.\n",
    "- Continuous Bag of Words: train a network to predict a missing word by using all the words around and during this process, the network will learn and update embeddings for all the words involved.\n",
    "- Skip-gram model: take one word and predict the context words instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creat a function that take all the individual word vectors in a give sentences and create a normalized vector from all the word vectors of the tokens.\n",
    "\n",
    "def sentence_to_vec(s, embedding_dict, stop_words, tokenizer):\n",
    "    words = str(s).lower()\n",
    "    words = tokenizer(words)\n",
    "    \n",
    "    # remove stop words\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    # keep only alpha numeric tokens\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        if w in embedding_dict:\n",
    "            M.append(embedding_dict[w])\n",
    "    if len(M) == 0:\n",
    "        return np.zeros(300)\n",
    "    M = np.array(M)\n",
    "    \n",
    "    # convert the list of embeddings to array\n",
    "    v = M.sum(axis=0)\n",
    "    \n",
    "    # return normalized vector\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector for the sentence:\n",
      "[0.04546079 0.06171939 0.05681235 0.09107364 0.04482545 0.07707392\n",
      " 0.05248489 0.02372561 0.07575552 0.04421887 0.04683005 0.09040362\n",
      " 0.06337044 0.07052759 0.03960564 0.06728026 0.04897506 0.06118425\n",
      " 0.07011926 0.07556736 0.05487185 0.06563027 0.02030972 0.07034759\n",
      " 0.07748291 0.03972214 0.08640887 0.01519241 0.04074792 0.04309249\n",
      " 0.04309843 0.08706945 0.04783806 0.03630182 0.05519518 0.07638176\n",
      " 0.06668437 0.06510008 0.06726166 0.04505232 0.02989269 0.04951909\n",
      " 0.04510504 0.0422993  0.06033995 0.02451907 0.0543995  0.0161094\n",
      " 0.02797804 0.0334433  0.07792725 0.07308441 0.0216232  0.05297646\n",
      " 0.0503801  0.017344   0.05875385 0.02622825 0.04870248 0.0581891\n",
      " 0.05735836 0.04680813 0.06153573 0.05533239 0.03790741 0.05766763\n",
      " 0.04834462 0.06681266 0.0397839  0.05850705 0.07629182 0.04673489\n",
      " 0.03562533 0.08619944 0.06604801 0.06143598 0.05828666 0.07390043\n",
      " 0.04049738 0.05028619 0.07969187 0.05435471 0.04871565 0.03611747\n",
      " 0.093696   0.04226273 0.09510693 0.03944804 0.06601327 0.06150058\n",
      " 0.04231288 0.05731178 0.03995608 0.03358351 0.04104377 0.09852238\n",
      " 0.04932878 0.02476073 0.07792753 0.08276789 0.05499785 0.03870939\n",
      " 0.06139859 0.08753824 0.04889135 0.05395513 0.03191931 0.06721167\n",
      " 0.03610759 0.05719676 0.05440494 0.02301344 0.06177281 0.06117637\n",
      " 0.07222478 0.07007962 0.09964589 0.0647848  0.07611729 0.03702343\n",
      " 0.05599541 0.05730549 0.07534689 0.06594172 0.07732139 0.05898797\n",
      " 0.05932017 0.04133244 0.0194108  0.05509058 0.08748788 0.03060986\n",
      " 0.01329333 0.04257512 0.0298804  0.04450048 0.05367631 0.02557641\n",
      " 0.0524375  0.07107499 0.05289396 0.06206854 0.04109711 0.05192451\n",
      " 0.02509119 0.04834037 0.06981792 0.05628686 0.06626051 0.09306945\n",
      " 0.04011312 0.05123046 0.06301689 0.06140536 0.06362842 0.06471428\n",
      " 0.02264959 0.078997   0.05291119 0.07063768 0.05510401 0.06255361\n",
      " 0.07351392 0.04098839 0.08298948 0.05547443 0.0609628  0.03434286\n",
      " 0.04756251 0.0750852  0.08120052 0.06521904 0.02977945 0.08170994\n",
      " 0.06984119 0.05691155 0.03894811 0.06099835 0.01791281 0.0304574\n",
      " 0.05406389 0.01752312 0.06270343 0.04733469 0.07428514 0.03753613\n",
      " 0.06653568 0.04434038 0.06414014 0.07107817 0.04552982 0.03140922\n",
      " 0.07987442 0.05816559 0.06693881 0.06691981 0.03383735 0.07271781\n",
      " 0.03756897 0.05578513 0.01834261 0.04926126 0.03342224 0.03542306\n",
      " 0.06261758 0.07802052 0.04308273 0.05362225 0.0521683  0.04446586\n",
      " 0.06119088 0.03958767 0.03005305 0.06017323 0.04460613 0.03921115\n",
      " 0.06411645 0.05918161 0.04038566 0.06282796 0.02529878 0.07090912\n",
      " 0.0682325  0.05405335 0.04771928 0.06879438 0.05178785 0.03084201\n",
      " 0.04728277 0.05270744 0.04440513 0.06225194 0.04913288 0.05881769\n",
      " 0.04650055 0.02734993 0.08397137 0.0430674  0.07174309 0.03612363\n",
      " 0.0501355  0.04360607 0.07330206 0.05594747 0.03114112 0.04552424\n",
      " 0.02955892 0.08715241 0.06299594 0.03184298 0.06937754 0.02634675\n",
      " 0.04838331 0.08267778 0.07025058 0.0222388  0.05388578 0.06110773\n",
      " 0.04392214 0.06412961 0.06345456 0.04249987 0.05455599 0.0698038\n",
      " 0.06815905 0.03175902 0.0817328  0.08359123 0.05371342 0.0521438\n",
      " 0.06046596 0.05842141 0.06237491 0.07692644 0.05562534 0.06161286\n",
      " 0.06775432 0.0499664  0.04495806 0.09695648 0.0471725  0.06555913\n",
      " 0.0715708  0.09344377 0.05589739 0.08379312 0.01259372 0.09173457\n",
      " 0.0336659  0.04937395 0.06344543 0.05445308 0.02474318 0.02338317\n",
      " 0.06393549 0.05370228 0.08109042 0.0363159  0.03089618 0.08605781]\n"
     ]
    }
   ],
   "source": [
    "## application of the function\n",
    "# Sample embedding dictionary\n",
    "embedding_dict = {\n",
    "    \"the\": np.random.rand(300),\n",
    "    \"cat\": np.random.rand(300),\n",
    "    \"sat\": np.random.rand(300),\n",
    "    \"on\": np.random.rand(300),\n",
    "    \"mat\": np.random.rand(300)\n",
    "}\n",
    "\n",
    "# Stop words\n",
    "stop_words = {\"the\", \"on\"}\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"The cat sat on the mat.\"\n",
    "\n",
    "# Convert sentence to vector\n",
    "vector = sentence_to_vec(sentence, embedding_dict, stop_words, word_tokenize)\n",
    "print(\"Embedding vector for the sentence:\")\n",
    "print(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chand\\AppData\\Local\\Temp\\ipykernel_23036\\888311115.py:9: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  cosine_similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"I love programming.\"\n",
    "sentence2 = \"Coding is my passion.\"\n",
    "\n",
    "# Generate vectors\n",
    "vec1 = sentence_to_vec(sentence1, embedding_dict, stop_words, word_tokenize)\n",
    "vec2 = sentence_to_vec(sentence2, embedding_dict, stop_words, word_tokenize)\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "print(\"Cosine Similarity:\", cosine_similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text data is just like time series data. \n",
    "We can use Long short term memory (LSTM) or Gated Recurrent Units (GRU) or CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # read training data\n",
    "    df = pd.read_csv(r\"C:\\Users\\Chand\\OneDrive\\桌面\\ML Project\\NLP\\imdb.csv\")\n",
    "    \n",
    "    df.sentiment = df.sentiment.apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "    \n",
    "    df[\"kfold\"] = -1\n",
    "    \n",
    "    df = df.sample(frac=1).reset_index(drop=True)   \n",
    "    \n",
    "    y = df.sentiment.values\n",
    "    \n",
    "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "        df.loc[v_, 'kfold'] = f\n",
    "        \n",
    "    df.to_csv(r\"C:\\Users\\Chand\\OneDrive\\桌面\\ML Project\\NLP\\imdb_folds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset:\n",
    "    def __init__(self, reviews, targets):\n",
    "        \"\"\"\n",
    "        :param reviews: list of strings\n",
    "        :param targets: list of integers\n",
    "        \"\"\"\n",
    "        self.reviews = reviews\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        review = self.reviews[item]\n",
    "        target = self.targets[item]\n",
    "        \n",
    "        return {\n",
    "            \"review\": torch.tensor(review, dtype=torch.long),\n",
    "            \"target\": torch.tensor(target, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review': tensor([13,  4,  8, 24,  6, 27, 22, 21]), 'target': tensor(1.)}\n"
     ]
    }
   ],
   "source": [
    "# Example reviews and targets\n",
    "example_reviews = [\n",
    "    \"The movie was fantastic and I loved it.\",\n",
    "    \"It was a terrible movie, not worth watching.\",\n",
    "    \"An average film with decent acting and plot.\",\n",
    "    \"Absolutely loved the direction and storyline.\",\n",
    "    \"The movie was too slow and boring.\"\n",
    "]\n",
    "example_targets = [1, 0, 1, 1, 0]\n",
    "\n",
    "# Simple word-to-index tokenizer\n",
    "word_to_index = {\n",
    "    word: i + 1 for i, word in enumerate(set(\" \".join(example_reviews).split()))\n",
    "}\n",
    "\n",
    "def tokenize(review):\n",
    "    return [word_to_index[word] for word in review.split()]\n",
    "\n",
    "# Updated IMDBDataset class\n",
    "class IMDBDataset:\n",
    "    def __init__(self, reviews, targets):\n",
    "        self.reviews = [tokenize(review) for review in reviews]\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        review = self.reviews[item]\n",
    "        target = self.targets[item]\n",
    "        \n",
    "        return {\n",
    "            \"review\": torch.tensor(review, dtype=torch.long),\n",
    "            \"target\": torch.tensor(target, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Create dataset instance\n",
    "dataset = IMDBDataset(example_reviews, example_targets)\n",
    "\n",
    "# Access a sample\n",
    "sample = dataset[0]\n",
    "print(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        # number of words = number of rows in the embedding matrix\n",
    "        num_words = embedding_matrix.shape[0]\n",
    "        \n",
    "        #dimension of the embedding matrix = number of columns in the embedding matrix\n",
    "        embed_dim = embedding_matrix.shape[1]\n",
    "        \n",
    "        # define an input embedding layer\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=num_words,\n",
    "            embedding_dim=embed_dim\n",
    "        )\n",
    "        \n",
    "        # set the weights of the embedding layer\n",
    "        self.embedding.weight = nn.Parameter(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "        )\n",
    "        \n",
    "        # don't update the weights of the embedding layer\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            128,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Linear(512, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # pass the inputs through the embedding layer\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # pass the embeddings through LSTM layer\n",
    "        h_lstm, _ = self.lstm(x)\n",
    "        \n",
    "        avg_pool = torch.mean(h_lstm, 1)\n",
    "        max_pool, _ = torch.max(h_lstm, 1)\n",
    "        \n",
    "        out = torch.cat((avg_pool, max_pool), 1)\n",
    "        out = self.out(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
